# Karla Implementation Plan

## Current Status

Karla is a Python-based coding agent using Letta's `kv_cache_friendly=True` for efficient KV cache reuse.

### Completed
- âœ… Core client-side tool execution (Read, Write, Edit, Bash, BashOutput, KillBash, Grep, Glob)
- âœ… Planning tools (EnterPlanMode, ExitPlanMode, TodoWrite)
- âœ… Agent tools (Task, Skill, AskUserQuestion, TaskOutput)
- âœ… Tool registration with Letta via Python stubs + JSON schemas
- âœ… System prompts (`prompts/karla_main.md`, `prompts/persona.md`)
- âœ… CLI headless mode (`karla "prompt"`, `--continue`, `--new`, `--agent`)
- âœ… Settings persistence (`~/.karla/settings.json`, `.karla/settings.local.json`)
- âœ… Memory blocks (persona, human, project, skills, loaded_skills)
- âœ… Memory tool attachment (unified `memory` tool)
- âœ… E2E tests verifying full loop works

### In Progress
- ðŸ”„ Slash commands implementation (see `SLASH_COMMANDS_PLAN.md`)
- ðŸ”„ Interactive CLI mode

## Next Phase: Slash Commands

See `SLASH_COMMANDS_PLAN.md` for detailed implementation plan.

### Priority Commands
| Command | Purpose | Status |
|---------|---------|--------|
| `/clear` | Reset conversation (triggers prompt re-render) | TODO |
| `/compact` | Summarize conversation | TODO |
| `/remember` | Store info to memory blocks | TODO |
| `/init` | Initialize agent memory from project | TODO |
| `/memory` | View current memory blocks | TODO |
| `/help` | List available commands | TODO |

### Interactive Mode
Currently Karla only supports:
- Headless mode: `karla "prompt"`
- Tool testing REPL: `karla repl`

Need to add:
- Interactive chat mode: `karla chat` or `karla -i`
- Slash command support in interactive mode

## Architecture Notes

### kv_cache_friendly
- System prompt stays STATIC during normal conversation
- Memory tool writes go to database, not live prompt
- Prompt updates happen on: `/clear`, `/compact`, new session
- This is intentional - different from letta-code's approach

### Memory Philosophy
- `memory_read`: Agent checks current block state (may differ from snapshot)
- `memory_insert`/`memory_replace`: Writes queue for next session
- `/remember`, `/init`: User-initiated prompt updates

### Tool Execution
- Tools registered as Python stubs on Letta server
- Actual execution happens client-side
- Results sent back via approval flow
- Letta never runs our code

## File Structure

```
src/karla/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ __main__.py           # python -m karla support
â”œâ”€â”€ cli.py                # CLI entry point
â”œâ”€â”€ config.py             # Configuration loading
â”œâ”€â”€ executor.py           # Tool execution
â”œâ”€â”€ registry.py           # Tool registry
â”œâ”€â”€ tool.py               # Tool base class
â”œâ”€â”€ letta.py              # Letta integration
â”œâ”€â”€ memory.py             # Memory block utilities
â”œâ”€â”€ settings.py           # Settings persistence
â”œâ”€â”€ skills.py             # Skill discovery
â”œâ”€â”€ agent_loop.py         # Main agent loop
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ karla_main.md     # System prompt
â”‚   â”œâ”€â”€ persona.md        # Persona block
â”‚   â”œâ”€â”€ human.md          # Human block
â”‚   â”œâ”€â”€ project.md        # Project block
â”‚   â””â”€â”€ memory_blocks.py
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ read.py, write.py, edit.py
â”‚   â”œâ”€â”€ bash.py, bash_background.py
â”‚   â”œâ”€â”€ grep.py, glob.py
â”‚   â”œâ”€â”€ plan_mode.py, todo.py
â”‚   â”œâ”€â”€ task.py, skill.py, ask_user.py
â”‚   â””â”€â”€ ...
â””â”€â”€ commands/             # TODO: Slash commands
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ registry.py
    â”œâ”€â”€ core.py
    â””â”€â”€ prompts.py
```

## Testing

```bash
# Unit tests
uv run pytest tests/ -x -q

# Integration tests (requires Letta server)
uv run pytest tests/test_integration.py -v

# Manual testing
uv run karla "Create hello.py that prints Hello World"
uv run karla --continue "Add a greet function"
uv run karla list
uv run karla repl
```

## Configuration

```yaml
# karla.yaml
server:
  base_url: http://localhost:8283
  timeout: null  # No timeout for local LLMs

llm:
  model: your-model-here
  model_endpoint: http://your-endpoint/v1
  model_endpoint_type: openai
  context_window: 8000

embedding:
  model: ollama/mxbai-embed-large:latest

agent_defaults:
  kv_cache_friendly: true
  include_base_tools: true
```
